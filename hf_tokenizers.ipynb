{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hf_tokenizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagyeomJung/deeplearning_master/blob/main/hf_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynCMLZQqgWHq"
      },
      "source": [
        "## Training SOTA tokenizer models using HuggingFace `tokenizers` package\n",
        "\n",
        "1. Word Level\n",
        "2. BPE - Byte Pair Encoder\n",
        "3. Unigram\n",
        "4. Word Piece"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96Z883c-jjhT",
        "outputId": "74e09bc1-320f-4991-a0c4-36d43fa1d848"
      },
      "source": [
        "!pip install tokenizers\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77_XrM_Vgq8M"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlEbro2djoFY"
      },
      "source": [
        "## importing the tokenizer and subword BPE trainer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
        "                                WordPieceTrainer, UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "untCq1TSl20J"
      },
      "source": [
        "#### Download the data to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRAZgeTkolcn",
        "outputId": "3f761f7e-6530-4b3e-b5ab-2f5252b12c76"
      },
      "source": [
        "!wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-17 10:16:15--  http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/16457/pg16457.txt [following]\n",
            "--2021-10-17 10:16:15--  https://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 617622 (603K) [text/plain]\n",
            "Saving to: ‚Äòpg16457.txt.2‚Äô\n",
            "\n",
            "pg16457.txt.2       100%[===================>] 603.15K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-10-17 10:16:16 (5.45 MB/s) - ‚Äòpg16457.txt.2‚Äô saved [617622/617622]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNPhkPE2nIIG",
        "outputId": "70f1efb0-719b-47d0-8cbf-58a82d16a3d9"
      },
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-17 10:16:59--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.229\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.229|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòwikitext-103-raw-v1.zip.3‚Äô\n",
            "\n",
            " wikitext-103-raw-v  71%[=============>      ] 130.65M  65.4MB/s               ^C\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "replace wikitext-103-raw/wiki.test.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "replace wikitext-103-raw/wiki.valid.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "replace wikitext-103-raw/wiki.train.raw? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wikitext-103-raw/wiki.train.raw  y\n",
            "y\n",
            "y\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6GxCTfj0OWw"
      },
      "source": [
        "## Define the 3-step process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFmfGYSfrT5y"
      },
      "source": [
        "unk_token = \"<UNK>\"  # token for unknown words\n",
        "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
        "\n",
        "def prepare_tokenizer_trainer(alg):\n",
        "    \"\"\"\n",
        "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
        "    \"\"\"\n",
        "    if alg == 'BPE':\n",
        "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
        "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
        "    elif alg == 'UNI':\n",
        "        tokenizer = Tokenizer(Unigram())\n",
        "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
        "    elif alg == 'WPC':\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
        "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
        "    else:\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
        "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
        "    \n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    return tokenizer, trainer\n",
        "\n",
        "\n",
        "def train_tokenizer(files, alg='WLV'):\n",
        "    \"\"\"\n",
        "    Takes the files and trains the tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
        "    tokenizer.train(files, trainer) # training the tokenzier\n",
        "    tokenizer.save(\"./tokenizer-trained.json\")\n",
        "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
        "    return tokenizer\n",
        "\n",
        "def tokenize(input_string, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizes the input string using the tokenizer provided.\n",
        "    \"\"\"\n",
        "    output = tokenizer.encode(input_string)\n",
        "    return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP1xuStV0fDl"
      },
      "source": [
        "## Training each model on the small as well as the large dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWb05mfxv1jw",
        "outputId": "8677e9b5-5fe4-4be5-d1cc-0c3430e5a55f"
      },
      "source": [
        "##training on a small dataset\n",
        "small_file = ['pg16457.txt']\n",
        "large_files = [f\"./wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "\n",
        "tokens_dict = {}\n",
        "\n",
        "for files in [small_file, large_files]:\n",
        "    print(f\"========Using vocabulary from {files}=======\")\n",
        "    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
        "        trained_tokenizer = train_tokenizer(files, alg)\n",
        "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç\"\n",
        "        output = tokenize(input_string, trained_tokenizer)\n",
        "        tokens_dict[alg] = output.tokens\n",
        "        print(\"----\", alg, \"----\")\n",
        "        print(output.tokens, \"->\", len(output.tokens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========Using vocabulary from ['pg16457.txt']=======\n",
            "---- WLV ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'tokenization', 'tutorial', '.', 'Tokenization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NLP', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tokens', 'generated', 'by', 'each', 'tokenization', 'model', '.', 'Excited', 'much', '?!üòç'] -> 35\n",
            "---- BPE ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 't', 'ut', 'or', 'ial', '.', 'T', 'ok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', 'k', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'c', 'ited', 'much', '?', '!', '<UNK>'] -> 55\n",
            "---- UNI ----\n",
            "['Thi', 's', 'is', 'a', 'deep', 'learn', 'ing', 'to', 'ken', 'iz', 'ation', 't', 'u', 'to', 'rial', '.', 'To', 'ken', 'iz', 'ation', 'is', 'the', 'fir', 's', 't', 'step', 'in', 'a', 'deep', 'learn', 'ing', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'compar', 'ing', 'the', 'to', 'ken', 's', 'generat', 'ed', 'by', 'each', 'to', 'ken', 'iz', 'ation', 'model', '.', 'E', 'x', 'c', 'it', 'ed', 'm', 'uch', '?', '!', 'üòç'] -> 67\n",
            "---- WPC ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 't', '##ut', '##oria', '##l', '.', 'To', '##ken', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', '##L', '##P', 'pip', '##el', '##ine', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Ex', '##ci', '##ted', 'much', '<UNK>'] -> 52\n",
            "========Using vocabulary from ['./wikitext-103-raw/wiki.test.raw', './wikitext-103-raw/wiki.train.raw', './wikitext-103-raw/wiki.valid.raw']=======\n",
            "---- WLV ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'tokenization', 'tutorial', '.', 'Tokenization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NLP', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tokens', 'generated', 'by', 'each', 'tokenization', 'model', '.', 'Excited', 'much', '?!üòç'] -> 35\n",
            "---- BPE ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 'tut', 'orial', '.', 'Tok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', 'P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tok', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'cited', 'much', '?', '!', '<UNK>'] -> 47\n",
            "---- UNI ----\n",
            "['This', 'i', 's', 'a', 'deep', 'learn', 'ing', 't', 'o', 'ken', 'ization', 't', 'u', 't', 'o', 'rial', '.', 'T', 'o', 'ken', 'ization', 'i', 's', 'the', 'first', 'step', 'in', 'a', 'deep', 'learn', 'ing', 'N', 'L', 'P', 'p', 'i', 'p', 'e', 'line', '.', 'W', 'e', 'will', 'be', 'com', 'par', 'ing', 'the', 't', 'o', 'ken', 's', 'generate', 'd', 'by', 'each', 't', 'o', 'ken', 'ization', 'model', '.', 'Ex', 'cited', 'much', '?', '!', 'üòç'] -> 68\n",
            "---- WPC ----\n",
            "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 'tut', '##orial', '.', 'Tok', '##eni', '##za', '##ti', '##on', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', '##P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Exc', '##ited', 'much', '<UNK>'] -> 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNMPi3802ac2"
      },
      "source": [
        "## Comparing the BPE and Unigram tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUYLXYt626O2"
      },
      "source": [
        "\n",
        "tokens_dict = {}\n",
        "\n",
        "for alg in ['BPE', 'UNI', 'WPC']:\n",
        "    trained_tokenizer = train_tokenizer(large_files, alg)\n",
        "    input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!üòç\"\n",
        "    output = tokenize(input_string, trained_tokenizer)\n",
        "    tokens_dict[alg] = output.tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "hZm0Za-v7l0l",
        "outputId": "92ba17db-d147-4865-f85c-e0e9bd7357dd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "max_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))\n",
        "diff_bpe = max_len - len(tokens_dict['BPE'])\n",
        "diff_wpc = max_len - len(tokens_dict['WPC'])\n",
        "\n",
        "tokens_dict['BPE'] = tokens_dict['BPE'] + ['<PAD>']*diff_bpe\n",
        "tokens_dict['WPC'] = tokens_dict['WPC'] + ['<PAD>']*diff_wpc\n",
        "\n",
        "del tokens_dict['WLV']\n",
        "\n",
        "df = pd.DataFrame(tokens_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68\n",
            "68\n",
            "68\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BPE</th>\n",
              "      <th>UNI</th>\n",
              "      <th>WPC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>s</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>deep</td>\n",
              "      <td>a</td>\n",
              "      <td>deep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>learning</td>\n",
              "      <td>deep</td>\n",
              "      <td>learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>cited</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>much</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>?</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>!</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>üòç</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         BPE    UNI       WPC\n",
              "0       This   This      This\n",
              "1         is      i        is\n",
              "2          a      s         a\n",
              "3       deep      a      deep\n",
              "4   learning   deep  learning\n",
              "..       ...    ...       ...\n",
              "63     <PAD>  cited     <PAD>\n",
              "64     <PAD>   much     <PAD>\n",
              "65     <PAD>      ?     <PAD>\n",
              "66     <PAD>      !     <PAD>\n",
              "67     <PAD>      üòç     <PAD>\n",
              "\n",
              "[68 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "Pq4HsfdhBU-N",
        "outputId": "57d636ef-21b9-42a1-9dc3-14ead705a9a0"
      },
      "source": [
        "df.head(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BPE</th>\n",
              "      <th>UNI</th>\n",
              "      <th>WPC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "      <td>This</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>s</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>deep</td>\n",
              "      <td>a</td>\n",
              "      <td>deep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>learning</td>\n",
              "      <td>deep</td>\n",
              "      <td>learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>to</td>\n",
              "      <td>learn</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ken</td>\n",
              "      <td>ing</td>\n",
              "      <td>##ken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ization</td>\n",
              "      <td>t</td>\n",
              "      <td>##ization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tut</td>\n",
              "      <td>o</td>\n",
              "      <td>tut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>orial</td>\n",
              "      <td>ken</td>\n",
              "      <td>##orial</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        BPE    UNI        WPC\n",
              "0      This   This       This\n",
              "1        is      i         is\n",
              "2         a      s          a\n",
              "3      deep      a       deep\n",
              "4  learning   deep   learning\n",
              "5        to  learn         to\n",
              "6       ken    ing      ##ken\n",
              "7   ization      t  ##ization\n",
              "8       tut      o        tut\n",
              "9     orial    ken    ##orial"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "lGMUMZeS4YQD",
        "outputId": "1c9f9694-7f5e-4844-c90d-668932e134fc"
      },
      "source": [
        "df.describe(include= 'all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BPE</th>\n",
              "      <th>UNI</th>\n",
              "      <th>WPC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>37</td>\n",
              "      <td>41</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "      <td>o</td>\n",
              "      <td>&lt;PAD&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          BPE UNI    WPC\n",
              "count      68  68     68\n",
              "unique     37  41     37\n",
              "top     <PAD>   o  <PAD>\n",
              "freq       21   5     20"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTCeB6Ka8p2n",
        "outputId": "8a735355-03cb-4935-bc76-d827d18b47c6"
      },
      "source": [
        "set(df['UNI']) - set(df['BPE'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'L',\n",
              " 'N',\n",
              " 'T',\n",
              " 'W',\n",
              " 'com',\n",
              " 'd',\n",
              " 'e',\n",
              " 'generate',\n",
              " 'i',\n",
              " 'ing',\n",
              " 'learn',\n",
              " 'line',\n",
              " 'o',\n",
              " 'p',\n",
              " 'par',\n",
              " 'rial',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'üòç'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJrxr9uW_8Gf",
        "outputId": "956ef38c-71c9-4c1f-a760-af1ae3adfae2"
      },
      "source": [
        "set(df['UNI']) - set(df['WPC'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!',\n",
              " '?',\n",
              " 'Ex',\n",
              " 'L',\n",
              " 'N',\n",
              " 'P',\n",
              " 'T',\n",
              " 'W',\n",
              " 'cited',\n",
              " 'com',\n",
              " 'd',\n",
              " 'e',\n",
              " 'generate',\n",
              " 'i',\n",
              " 'ing',\n",
              " 'ization',\n",
              " 'ken',\n",
              " 'learn',\n",
              " 'line',\n",
              " 'o',\n",
              " 'p',\n",
              " 'par',\n",
              " 'rial',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'üòç'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhKiRtkFHW_P",
        "outputId": "d2dd45db-7f26-465d-ab65-c424e199abbe"
      },
      "source": [
        "set(df['WPC']) - set(df['UNI'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'##P',\n",
              " '##eni',\n",
              " '##ited',\n",
              " '##ization',\n",
              " '##ken',\n",
              " '##on',\n",
              " '##orial',\n",
              " '##s',\n",
              " '##ti',\n",
              " '##za',\n",
              " '<PAD>',\n",
              " '<UNK>',\n",
              " 'Exc',\n",
              " 'NL',\n",
              " 'Tok',\n",
              " 'We',\n",
              " 'comparing',\n",
              " 'generated',\n",
              " 'is',\n",
              " 'learning',\n",
              " 'pipeline',\n",
              " 'to',\n",
              " 'tut'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYbFvw4Z12fj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}