{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq , 어텐션 메커니즘.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjFBmKX5KJUolDkSKgbz/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagyeom23658/deeplearning_master/blob/main/seq2seq_%2C_%EC%96%B4%ED%85%90%EC%85%98_%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN을 이용한 인코더-디코더\n",
        "\n",
        "-  케라스 함수형 API(https://wikidocs.net/38861)"
      ],
      "metadata": {
        "id": "dbTEmt9-_peY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 시퀀스-투-시퀀스(Sequence-to-Sequence)\n",
        "\n",
        "시퀀스-투-시퀀스(Sequence-to-Sequence)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델입니다. 예를 들어 챗봇(Chatbot)과 기계 번역(Machine Translation)이 그러한 대표적인 예인데, 입력 시퀀스와 출력 시퀀스를 각각 질문과 대답으로 구성하면 챗봇으로 만들 수 있고, 입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만들면 번역기로 만들 수 있습니다. 그 외에도 내용 요약(Text Summarization), STT(Speech to Text) 등에서 쓰일 수 있습니다.\n",
        "\n",
        "- 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 이를 컨텍스트 벡터(context vector)라고 합니다. 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송합니다. 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력합니다.\n",
        "\n",
        "- 인코더 아키텍처와 디코더 아키텍처의 내부는 사실 두 개의 RNN 아키텍처 입니다. 입력 문장을 받는 RNN 셀을 인코더라고 하고, 출력 문장을 출력하는 RNN 셀을 디코더라고 합니다.\n",
        "\n",
        "- 인코더 RNN 셀은 모든 단어를 입력받은 뒤에 인코더 RNN 셀의 마지막 시점의 은닉 상태를 디코더 RNN 셀로 넘겨주는데 이를 컨텍스트 벡터라고 합니다. 컨텍스트 벡터는 디코더 RNN 셀의 첫번째 은닉 상태로 사용됩니다.\n",
        "\n",
        "- 디코더는 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어갑니다. 디코더는 <sos>가 입력되면, 다음에 등장할 확률이 높은 단어를 예측합니다. 첫번째 시점(time step)의 디코더 RNN 셀은 다음에 등장할 단어로 je를 예측하였습니다. 첫번째 시점의 디코더 RNN 셀은 예측된 단어 je를 다음 시점의 RNN 셀의 입력으로 입력합니다. 그리고 두번째 시점의 디코더 RNN 셀은 입력된 단어 je로부터 다시 다음에 올 단어인 suis를 예측하고, 또 다시 이것을 다음 시점의 RNN 셀의 입력으로 보냅니다. 디코더는 이런 식으로 기본적으로 다음에 올 단어를 예측하고, 그 예측한 단어를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복합니다. 이 행위는 문장의 끝을 의미하는 심볼인 <eos>가 다음 단어로 예측될 때까지 반복됩니다.\n",
        "\n",
        "- 출력 단어로 나올 수 있는 단어들은 다양한 단어들이 있습니다. seq2seq 모델은 선택될 수 있는 모든 단어들로부터 하나의 단어를 골라서 예측해야 합니다.  디코더에서 각 시점(time step)의 RNN 셀에서 출력 벡터가 나오면, 해당 벡터는 소프트맥스 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정합니다."
      ],
      "metadata": {
        "id": "Udaywl07ABGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 글자 레벨 기계 번역기(Character-Level Neural Machine Translation) 구현하기\n",
        "sequence-to-sequence 10분만에 이해하기 : https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n",
        "기계 번역기를 훈련시키기 위해서는 훈련 데이터로 병렬 코퍼스(parallel corpus)가 필요합니다. 병렬 코퍼스란, 두 개 이상의 언어가 병렬적으로 구성된 코퍼스를 의미합니다.\n",
        "\n",
        "https://lsjsj92.tistory.com/493"
      ],
      "metadata": {
        "id": "X2c3I2fJFUOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zWKfIhU-niiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import urllib3\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "FzN5wuNVAA7M"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/kor-eng.zip'\n",
        "filename = 'kor-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "RT9A9b-YGbxV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = pd.read_csv('kor.txt', names=['src', 'tar', 'lic'], sep='\\t')  #src는 source의 줄임말로 입력 문장을 나타내며, tar는 target의 줄임말로 번역하고자 하는 문장\n",
        "print(lines)\n",
        "del lines['lic']\n",
        "print('전체 샘플의 개수 :',len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWO7x-UA_vR2",
        "outputId": "1e39f996-deda-4674-d26d-9cb56ff82174"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    src  ...                                                lic\n",
            "0                                                   Go.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
            "1                                                   Hi.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
            "2                                                  Run!  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
            "3                                                  Run.  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #4...\n",
            "4                                                  Who?  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
            "...                                                 ...  ...                                                ...\n",
            "3725  Science fiction has undoubtedly been the inspi...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #6...\n",
            "3726  I started a new blog. I'll do my best not to b...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #1...\n",
            "3727  I think it's a shame that some foreign languag...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
            "3728  If someone who doesn't know your background sa...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
            "3729  Doubtless there exists in this world precisely...  ...  CC-BY 2.0 (France) Attribution: tatoeba.org #7...\n",
            "\n",
            "[3730 rows x 3 columns]\n",
            "전체 샘플의 개수 : 3730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = lines.loc[:, 'src':'tar']\n",
        "# lines = lines[0:60000] # 6만개만 저장\n",
        "lines.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "V4cOHWO9G7Fk",
        "outputId": "4f5793b2-77e2-4ac9-d496-5219b30457ac"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>We can buy it.</td>\n",
              "      <td>이건 우리가 살 수 있어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2683</th>\n",
              "      <td>I don't like sitting on the floor.</td>\n",
              "      <td>난 바닥에 앉는 건 별로야.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2314</th>\n",
              "      <td>Tom wants to avoid everything.</td>\n",
              "      <td>톰은 모든 걸 회피하고 싶어해.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Go ahead.</td>\n",
              "      <td>계속해.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1040</th>\n",
              "      <td>Please drive slowly.</td>\n",
              "      <td>천천히 운전하세요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>Tom cheated.</td>\n",
              "      <td>톰이 사기 쳤어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>Fire burns.</td>\n",
              "      <td>불타네.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>I want to see you smile.</td>\n",
              "      <td>난 당신이 웃는 걸 보고 싶어요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2260</th>\n",
              "      <td>I want to get there by subway.</td>\n",
              "      <td>지하철로 가고 싶어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>I think Mary is way cuter than Alice.</td>\n",
              "      <td>메리가 앨리스보다 좀 더 귀여운 것 같아.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        src                      tar\n",
              "555                          We can buy it.           이건 우리가 살 수 있어.\n",
              "2683     I don't like sitting on the floor.          난 바닥에 앉는 건 별로야.\n",
              "2314         Tom wants to avoid everything.        톰은 모든 걸 회피하고 싶어해.\n",
              "72                                Go ahead.                     계속해.\n",
              "1040                   Please drive slowly.               천천히 운전하세요.\n",
              "350                            Tom cheated.                톰이 사기 쳤어.\n",
              "214                             Fire burns.                     불타네.\n",
              "1447               I want to see you smile.       난 당신이 웃는 걸 보고 싶어요.\n",
              "2260         I want to get there by subway.              지하철로 가고 싶어.\n",
              "2948  I think Mary is way cuter than Alice.  메리가 앨리스보다 좀 더 귀여운 것 같아."
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역 문장에 해당되는 한국어 데이터는 앞서 배웠듯이 시작을 의미하는 심볼 <sos>과 종료를 의미하는 심볼 <eos>을 넣어주어야 합니다. \n",
        "# 여기서는 <sos>와 <eos> 대신 '\\t'를 시작 심볼, '\\n'을 종료 심볼로 간주하여 추가하고 다시 데이터를 출력해보겠습니다.\n",
        "\n",
        "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
        "lines.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "su0y3eJLHT-C",
        "outputId": "a5fda914-b2cc-4a69-a82a-852d1bb61cab"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1380</th>\n",
              "      <td>Tom is a great manager.</td>\n",
              "      <td>\\t 톰은 관리를 정말 잘해. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>Life is too short.</td>\n",
              "      <td>\\t 삶은 너무 짧네. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2672</th>\n",
              "      <td>Every country has its own history.</td>\n",
              "      <td>\\t 어떤 나라든 역사가 있다. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1170</th>\n",
              "      <td>This room is too big.</td>\n",
              "      <td>\\t 이 방은 너무 커요. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2815</th>\n",
              "      <td>It's not as difficult as you think.</td>\n",
              "      <td>\\t 그건 생각보단 어렵지 않아. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980</th>\n",
              "      <td>I live in a small apartment.</td>\n",
              "      <td>\\t 나는 작은 아파트에 살아요. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>Have you ever been drunk?</td>\n",
              "      <td>\\t 취해본 적 있어? \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2149</th>\n",
              "      <td>I've heard this joke already.</td>\n",
              "      <td>\\t 이 농담은 이미 들어봤어. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2293</th>\n",
              "      <td>Tom and Mary are both lawyers.</td>\n",
              "      <td>\\t 톰과 메리는 둘 다 변호사야. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2470</th>\n",
              "      <td>I know I'm going to learn a lot.</td>\n",
              "      <td>\\t 내가 많이 배우게 될 거라는 걸 알고 있어. \\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      src                             tar\n",
              "1380              Tom is a great manager.             \\t 톰은 관리를 정말 잘해. \\n\n",
              "860                    Life is too short.                 \\t 삶은 너무 짧네. \\n\n",
              "2672   Every country has its own history.            \\t 어떤 나라든 역사가 있다. \\n\n",
              "1170                This room is too big.               \\t 이 방은 너무 커요. \\n\n",
              "2815  It's not as difficult as you think.           \\t 그건 생각보단 어렵지 않아. \\n\n",
              "1980         I live in a small apartment.           \\t 나는 작은 아파트에 살아요. \\n\n",
              "1549            Have you ever been drunk?                 \\t 취해본 적 있어? \\n\n",
              "2149        I've heard this joke already.            \\t 이 농담은 이미 들어봤어. \\n\n",
              "2293       Tom and Mary are both lawyers.          \\t 톰과 메리는 둘 다 변호사야. \\n\n",
              "2470     I know I'm going to learn a lot.  \\t 내가 많이 배우게 될 거라는 걸 알고 있어. \\n"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 글자 집합 구축(토큰 단위가 단어가 아니라 글자)\n",
        "src_vocab = set()\n",
        "for line in lines.src: # 1줄씩 읽음\n",
        "    for char in line: # 1개의 글자씩 읽음\n",
        "        src_vocab.add(char)\n",
        "\n",
        "tar_vocab = set()   # set : https://wikidocs.net/16044\n",
        "for line in lines.tar: \n",
        "    for char in line:\n",
        "        tar_vocab.add(char)"
      ],
      "metadata": {
        "id": "OWRtnVhgIfnK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY8vxtb0Jw-I",
        "outputId": "4e842883-5be8-408e-df48-4cab2811de1a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ',\n",
              " '!',\n",
              " '\"',\n",
              " '$',\n",
              " '%',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'Y',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '°',\n",
              " 'ï'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNA8l70CI70C",
        "outputId": "1012ea3f-48fb-4a18-9769-55e16485283b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t',\n",
              " '\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '%',\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'H',\n",
              " 'M',\n",
              " 'N',\n",
              " 'T',\n",
              " 'a',\n",
              " 'd',\n",
              " 'h',\n",
              " 'i',\n",
              " 'm',\n",
              " 'o',\n",
              " 'p',\n",
              " 'r',\n",
              " 't',\n",
              " 'y',\n",
              " '°',\n",
              " '가',\n",
              " '각',\n",
              " '간',\n",
              " '갇',\n",
              " '갈',\n",
              " '감',\n",
              " '갑',\n",
              " '값',\n",
              " '갔',\n",
              " '강',\n",
              " '갖',\n",
              " '같',\n",
              " '개',\n",
              " '객',\n",
              " '갰',\n",
              " '걀',\n",
              " '걔',\n",
              " '거',\n",
              " '걱',\n",
              " '건',\n",
              " '걷',\n",
              " '걸',\n",
              " '검',\n",
              " '겁',\n",
              " '것',\n",
              " '게',\n",
              " '겐',\n",
              " '겠',\n",
              " '겨',\n",
              " '격',\n",
              " '겪',\n",
              " '견',\n",
              " '결',\n",
              " '겼',\n",
              " '경',\n",
              " '계',\n",
              " '고',\n",
              " '곡',\n",
              " '곤',\n",
              " '곧',\n",
              " '골',\n",
              " '곰',\n",
              " '곱',\n",
              " '곳',\n",
              " '공',\n",
              " '과',\n",
              " '관',\n",
              " '광',\n",
              " '괜',\n",
              " '괴',\n",
              " '굉',\n",
              " '교',\n",
              " '구',\n",
              " '국',\n",
              " '군',\n",
              " '굳',\n",
              " '굴',\n",
              " '굶',\n",
              " '굼',\n",
              " '굽',\n",
              " '궁',\n",
              " '권',\n",
              " '귀',\n",
              " '귄',\n",
              " '규',\n",
              " '그',\n",
              " '극',\n",
              " '근',\n",
              " '글',\n",
              " '금',\n",
              " '급',\n",
              " '긋',\n",
              " '긍',\n",
              " '기',\n",
              " '긴',\n",
              " '길',\n",
              " '깊',\n",
              " '까',\n",
              " '깎',\n",
              " '깐',\n",
              " '깔',\n",
              " '깜',\n",
              " '깡',\n",
              " '깨',\n",
              " '꺼',\n",
              " '꺾',\n",
              " '껍',\n",
              " '껏',\n",
              " '껐',\n",
              " '께',\n",
              " '껴',\n",
              " '꼈',\n",
              " '꼬',\n",
              " '꼴',\n",
              " '꼼',\n",
              " '꽃',\n",
              " '꽉',\n",
              " '꽤',\n",
              " '꾸',\n",
              " '꾼',\n",
              " '꿇',\n",
              " '꿈',\n",
              " '꿔',\n",
              " '꿨',\n",
              " '뀌',\n",
              " '끄',\n",
              " '끈',\n",
              " '끊',\n",
              " '끌',\n",
              " '끓',\n",
              " '끔',\n",
              " '끗',\n",
              " '끙',\n",
              " '끝',\n",
              " '끼',\n",
              " '낀',\n",
              " '낄',\n",
              " '낌',\n",
              " '나',\n",
              " '낙',\n",
              " '낚',\n",
              " '난',\n",
              " '날',\n",
              " '낡',\n",
              " '남',\n",
              " '납',\n",
              " '났',\n",
              " '낭',\n",
              " '낮',\n",
              " '낯',\n",
              " '내',\n",
              " '낸',\n",
              " '낼',\n",
              " '냄',\n",
              " '냈',\n",
              " '냉',\n",
              " '냐',\n",
              " '냥',\n",
              " '너',\n",
              " '넌',\n",
              " '널',\n",
              " '넓',\n",
              " '넘',\n",
              " '넛',\n",
              " '넣',\n",
              " '네',\n",
              " '넷',\n",
              " '녀',\n",
              " '녁',\n",
              " '년',\n",
              " '념',\n",
              " '녕',\n",
              " '노',\n",
              " '녹',\n",
              " '논',\n",
              " '놀',\n",
              " '농',\n",
              " '높',\n",
              " '놓',\n",
              " '놔',\n",
              " '놨',\n",
              " '뇌',\n",
              " '누',\n",
              " '눅',\n",
              " '눈',\n",
              " '눴',\n",
              " '뉴',\n",
              " '늄',\n",
              " '느',\n",
              " '늑',\n",
              " '는',\n",
              " '늘',\n",
              " '늙',\n",
              " '능',\n",
              " '늦',\n",
              " '니',\n",
              " '닌',\n",
              " '님',\n",
              " '다',\n",
              " '닥',\n",
              " '닦',\n",
              " '단',\n",
              " '닫',\n",
              " '달',\n",
              " '닮',\n",
              " '담',\n",
              " '답',\n",
              " '당',\n",
              " '대',\n",
              " '댔',\n",
              " '더',\n",
              " '덕',\n",
              " '던',\n",
              " '덜',\n",
              " '덤',\n",
              " '덥',\n",
              " '데',\n",
              " '덴',\n",
              " '도',\n",
              " '독',\n",
              " '돈',\n",
              " '돌',\n",
              " '돕',\n",
              " '동',\n",
              " '돼',\n",
              " '됐',\n",
              " '되',\n",
              " '된',\n",
              " '될',\n",
              " '됩',\n",
              " '두',\n",
              " '둑',\n",
              " '둔',\n",
              " '둘',\n",
              " '둠',\n",
              " '둬',\n",
              " '뒀',\n",
              " '뒤',\n",
              " '뒷',\n",
              " '드',\n",
              " '득',\n",
              " '든',\n",
              " '듣',\n",
              " '들',\n",
              " '듯',\n",
              " '등',\n",
              " '디',\n",
              " '딘',\n",
              " '딨',\n",
              " '따',\n",
              " '딱',\n",
              " '딸',\n",
              " '땅',\n",
              " '땋',\n",
              " '때',\n",
              " '떠',\n",
              " '떡',\n",
              " '떤',\n",
              " '떨',\n",
              " '떴',\n",
              " '떻',\n",
              " '또',\n",
              " '똑',\n",
              " '뚱',\n",
              " '뛰',\n",
              " '뜨',\n",
              " '뜰',\n",
              " '뜻',\n",
              " '라',\n",
              " '락',\n",
              " '란',\n",
              " '랄',\n",
              " '람',\n",
              " '랍',\n",
              " '랐',\n",
              " '랑',\n",
              " '래',\n",
              " '랜',\n",
              " '램',\n",
              " '랩',\n",
              " '랬',\n",
              " '략',\n",
              " '량',\n",
              " '러',\n",
              " '럭',\n",
              " '런',\n",
              " '럴',\n",
              " '럼',\n",
              " '럽',\n",
              " '렀',\n",
              " '렁',\n",
              " '렇',\n",
              " '레',\n",
              " '렌',\n",
              " '렛',\n",
              " '려',\n",
              " '력',\n",
              " '련',\n",
              " '렵',\n",
              " '렸',\n",
              " '령',\n",
              " '례',\n",
              " '로',\n",
              " '록',\n",
              " '론',\n",
              " '롭',\n",
              " '뢰',\n",
              " '료',\n",
              " '루',\n",
              " '룹',\n",
              " '류',\n",
              " '륙',\n",
              " '륜',\n",
              " '륭',\n",
              " '르',\n",
              " '른',\n",
              " '를',\n",
              " '름',\n",
              " '릅',\n",
              " '릎',\n",
              " '리',\n",
              " '린',\n",
              " '릴',\n",
              " '림',\n",
              " '립',\n",
              " '마',\n",
              " '막',\n",
              " '만',\n",
              " '많',\n",
              " '말',\n",
              " '맙',\n",
              " '맛',\n",
              " '망',\n",
              " '맞',\n",
              " '맡',\n",
              " '매',\n",
              " '맥',\n",
              " '맨',\n",
              " '맷',\n",
              " '머',\n",
              " '먹',\n",
              " '먼',\n",
              " '멀',\n",
              " '멈',\n",
              " '멋',\n",
              " '멍',\n",
              " '메',\n",
              " '멕',\n",
              " '멜',\n",
              " '며',\n",
              " '면',\n",
              " '명',\n",
              " '몇',\n",
              " '모',\n",
              " '목',\n",
              " '몰',\n",
              " '몸',\n",
              " '못',\n",
              " '묘',\n",
              " '무',\n",
              " '묵',\n",
              " '묶',\n",
              " '문',\n",
              " '묻',\n",
              " '물',\n",
              " '뭇',\n",
              " '뭐',\n",
              " '뭔',\n",
              " '뭘',\n",
              " '므',\n",
              " '미',\n",
              " '민',\n",
              " '믿',\n",
              " '밀',\n",
              " '밌',\n",
              " '밍',\n",
              " '밑',\n",
              " '바',\n",
              " '박',\n",
              " '밖',\n",
              " '반',\n",
              " '받',\n",
              " '발',\n",
              " '밝',\n",
              " '밟',\n",
              " '밤',\n",
              " '밥',\n",
              " '방',\n",
              " '배',\n",
              " '백',\n",
              " '뱀',\n",
              " '버',\n",
              " '벅',\n",
              " '번',\n",
              " '벌',\n",
              " '범',\n",
              " '법',\n",
              " '벗',\n",
              " '벙',\n",
              " '베',\n",
              " '벼',\n",
              " '벽',\n",
              " '변',\n",
              " '별',\n",
              " '병',\n",
              " '보',\n",
              " '복',\n",
              " '본',\n",
              " '볼',\n",
              " '봄',\n",
              " '봅',\n",
              " '봇',\n",
              " '봉',\n",
              " '봐',\n",
              " '봤',\n",
              " '부',\n",
              " '북',\n",
              " '분',\n",
              " '불',\n",
              " '붉',\n",
              " '붐',\n",
              " '붕',\n",
              " '붙',\n",
              " '브',\n",
              " '블',\n",
              " '비',\n",
              " '빈',\n",
              " '빌',\n",
              " '빙',\n",
              " '빛',\n",
              " '빠',\n",
              " '빨',\n",
              " '빴',\n",
              " '빵',\n",
              " '빼',\n",
              " '뺄',\n",
              " '뻐',\n",
              " '뻔',\n",
              " '뻤',\n",
              " '뽀',\n",
              " '뽑',\n",
              " '뿌',\n",
              " '뿐',\n",
              " '쁘',\n",
              " '쁜',\n",
              " '쁠',\n",
              " '삐',\n",
              " '사',\n",
              " '삭',\n",
              " '산',\n",
              " '살',\n",
              " '삶',\n",
              " '삼',\n",
              " '샀',\n",
              " '상',\n",
              " '새',\n",
              " '색',\n",
              " '샌',\n",
              " '생',\n",
              " '샤',\n",
              " '샴',\n",
              " '서',\n",
              " '석',\n",
              " '선',\n",
              " '설',\n",
              " '섬',\n",
              " '섭',\n",
              " '섯',\n",
              " '섰',\n",
              " '성',\n",
              " '세',\n",
              " '센',\n",
              " '셀',\n",
              " '셈',\n",
              " '셔',\n",
              " '셜',\n",
              " '셨',\n",
              " '소',\n",
              " '속',\n",
              " '손',\n",
              " '솔',\n",
              " '송',\n",
              " '쇠',\n",
              " '수',\n",
              " '숙',\n",
              " '순',\n",
              " '숟',\n",
              " '술',\n",
              " '숨',\n",
              " '쉬',\n",
              " '쉽',\n",
              " '슈',\n",
              " '스',\n",
              " '슨',\n",
              " '슬',\n",
              " '습',\n",
              " '승',\n",
              " '시',\n",
              " '식',\n",
              " '신',\n",
              " '실',\n",
              " '싫',\n",
              " '심',\n",
              " '십',\n",
              " '싱',\n",
              " '싶',\n",
              " '싸',\n",
              " '쌉',\n",
              " '써',\n",
              " '썩',\n",
              " '썼',\n",
              " '썽',\n",
              " '쏘',\n",
              " '쏠',\n",
              " '쏴',\n",
              " '쓰',\n",
              " '쓱',\n",
              " '쓴',\n",
              " '쓸',\n",
              " '씀',\n",
              " '씨',\n",
              " '씩',\n",
              " '씬',\n",
              " '씻',\n",
              " '아',\n",
              " '악',\n",
              " '안',\n",
              " '앉',\n",
              " '않',\n",
              " '알',\n",
              " '앓',\n",
              " '암',\n",
              " '압',\n",
              " '앗',\n",
              " '았',\n",
              " '앙',\n",
              " '앞',\n",
              " '애',\n",
              " '액',\n",
              " '앨',\n",
              " '앵',\n",
              " '야',\n",
              " '약',\n",
              " '얀',\n",
              " '얇',\n",
              " '양',\n",
              " '얗',\n",
              " '얘',\n",
              " '어',\n",
              " '억',\n",
              " '언',\n",
              " '얻',\n",
              " '얼',\n",
              " '엄',\n",
              " '업',\n",
              " '없',\n",
              " '엇',\n",
              " '었',\n",
              " '엌',\n",
              " '에',\n",
              " '엔',\n",
              " '엘',\n",
              " '여',\n",
              " '역',\n",
              " '연',\n",
              " '열',\n",
              " '염',\n",
              " '엽',\n",
              " '였',\n",
              " '영',\n",
              " '옆',\n",
              " '예',\n",
              " '옛',\n",
              " '오',\n",
              " '옥',\n",
              " '온',\n",
              " '올',\n",
              " '옮',\n",
              " '옳',\n",
              " '옷',\n",
              " '옹',\n",
              " '와',\n",
              " '완',\n",
              " '왔',\n",
              " '왜',\n",
              " '외',\n",
              " '왼',\n",
              " '요',\n",
              " '욕',\n",
              " '용',\n",
              " '우',\n",
              " '운',\n",
              " '울',\n",
              " '움',\n",
              " '웃',\n",
              " '워',\n",
              " '원',\n",
              " '월',\n",
              " '웠',\n",
              " '웨',\n",
              " '위',\n",
              " '윈',\n",
              " '윗',\n",
              " '윙',\n",
              " '유',\n",
              " '육',\n",
              " '윤',\n",
              " '으',\n",
              " '은',\n",
              " '을',\n",
              " '음',\n",
              " '읍',\n",
              " '응',\n",
              " '의',\n",
              " '이',\n",
              " '익',\n",
              " '인',\n",
              " '일',\n",
              " '읽',\n",
              " '잃',\n",
              " '임',\n",
              " '입',\n",
              " '있',\n",
              " '잊',\n",
              " '자',\n",
              " '작',\n",
              " '잔',\n",
              " '잖',\n",
              " '잘',\n",
              " '잠',\n",
              " '잡',\n",
              " '잤',\n",
              " '장',\n",
              " '재',\n",
              " '잭',\n",
              " '쟁',\n",
              " '저',\n",
              " '적',\n",
              " '전',\n",
              " '절',\n",
              " '젊',\n",
              " '점',\n",
              " '접',\n",
              " '정',\n",
              " '제',\n",
              " '젝',\n",
              " '젠',\n",
              " '젯',\n",
              " '져',\n",
              " '졌',\n",
              " '조',\n",
              " '족',\n",
              " '존',\n",
              " '졸',\n",
              " '좀',\n",
              " '종',\n",
              " '좋',\n",
              " '좌',\n",
              " '죄',\n",
              " '죠',\n",
              " '주',\n",
              " '죽',\n",
              " '준',\n",
              " '줄',\n",
              " '중',\n",
              " '줘',\n",
              " '줬',\n",
              " '쥐',\n",
              " '즈',\n",
              " '즉',\n",
              " '즐',\n",
              " '즘',\n",
              " '증',\n",
              " '지',\n",
              " '직',\n",
              " '진',\n",
              " '질',\n",
              " '집',\n",
              " '짓',\n",
              " '짖',\n",
              " '짜',\n",
              " '짝',\n",
              " '짧',\n",
              " '째',\n",
              " '쨌',\n",
              " '쩔',\n",
              " '쩡',\n",
              " '쪄',\n",
              " '쪘',\n",
              " '쪼',\n",
              " '쪽',\n",
              " '쫓',\n",
              " '쯤',\n",
              " '찌',\n",
              " '찍',\n",
              " '찔',\n",
              " '찡',\n",
              " '찢',\n",
              " '차',\n",
              " '착',\n",
              " '찬',\n",
              " '찮',\n",
              " '찰',\n",
              " '참',\n",
              " '찼',\n",
              " '창',\n",
              " '찾',\n",
              " '채',\n",
              " '책',\n",
              " '챌',\n",
              " '챘',\n",
              " '처',\n",
              " '척',\n",
              " '천',\n",
              " '철',\n",
              " '첫',\n",
              " '청',\n",
              " '체',\n",
              " '쳐',\n",
              " '쳤',\n",
              " '초',\n",
              " '촌',\n",
              " '총',\n",
              " '최',\n",
              " '추',\n",
              " '축',\n",
              " '출',\n",
              " '춤',\n",
              " '충',\n",
              " '춰',\n",
              " '췄',\n",
              " '취',\n",
              " '츠',\n",
              " '측',\n",
              " '치',\n",
              " '칙',\n",
              " '친',\n",
              " '칠',\n",
              " '침',\n",
              " '칩',\n",
              " '카',\n",
              " '캐',\n",
              " '커',\n",
              " '컨',\n",
              " '컴',\n",
              " '컵',\n",
              " '케',\n",
              " '켓',\n",
              " '켜',\n",
              " '켤',\n",
              " '켰',\n",
              " '코',\n",
              " '콜',\n",
              " '콥',\n",
              " '콩',\n",
              " '쾅',\n",
              " '쿠',\n",
              " '쿨',\n",
              " '퀴',\n",
              " '큐',\n",
              " '크',\n",
              " '큰',\n",
              " '큼',\n",
              " '키',\n",
              " '킬',\n",
              " '킹',\n",
              " '타',\n",
              " '탁',\n",
              " '탄',\n",
              " '탈',\n",
              " '탐',\n",
              " '탑',\n",
              " '탓',\n",
              " '탔',\n",
              " '탕',\n",
              " '태',\n",
              " '택',\n",
              " '터',\n",
              " '턱',\n",
              " '턴',\n",
              " '테',\n",
              " '텐',\n",
              " '토',\n",
              " '톤',\n",
              " '톰',\n",
              " '톱',\n",
              " '통',\n",
              " '퇴',\n",
              " '투',\n",
              " '트',\n",
              " '특',\n",
              " '튼',\n",
              " '틀',\n",
              " '티',\n",
              " '틱',\n",
              " '틴',\n",
              " '팀',\n",
              " '팅',\n",
              " '파',\n",
              " '판',\n",
              " '팔',\n",
              " '팠',\n",
              " '패',\n",
              " '퍼',\n",
              " '펐',\n",
              " '페',\n",
              " '펙',\n",
              " '펜',\n",
              " '펭',\n",
              " '펴',\n",
              " '편',\n",
              " '평',\n",
              " '폐',\n",
              " '포',\n",
              " '폭',\n",
              " '폰',\n",
              " '표',\n",
              " '푸',\n",
              " '푹',\n",
              " '풀',\n",
              " '품',\n",
              " '풍',\n",
              " '퓨',\n",
              " '프',\n",
              " '픈',\n",
              " '플',\n",
              " '픔',\n",
              " '피',\n",
              " '필',\n",
              " '핑',\n",
              " '하',\n",
              " '학',\n",
              " '한',\n",
              " '할',\n",
              " '함',\n",
              " '합',\n",
              " '항',\n",
              " '해',\n",
              " '핸',\n",
              " '햄',\n",
              " '했',\n",
              " '행',\n",
              " '향',\n",
              " '허',\n",
              " '헉',\n",
              " '헌',\n",
              " '험',\n",
              " '헤',\n",
              " '헬',\n",
              " '헷',\n",
              " '혀',\n",
              " '현',\n",
              " '혈',\n",
              " '혐',\n",
              " '협',\n",
              " '혔',\n",
              " '형',\n",
              " '혜',\n",
              " '호',\n",
              " '혹',\n",
              " '혼',\n",
              " '홀',\n",
              " '홋',\n",
              " '화',\n",
              " '확',\n",
              " '환',\n",
              " '활',\n",
              " '황',\n",
              " '회',\n",
              " '획',\n",
              " '효',\n",
              " '후',\n",
              " '훈',\n",
              " '훌',\n",
              " '훔',\n",
              " '훨',\n",
              " '휘',\n",
              " '휴',\n",
              " '흐',\n",
              " '흔',\n",
              " '흘',\n",
              " '흙',\n",
              " '흠',\n",
              " '흡',\n",
              " '흥',\n",
              " '희',\n",
              " '흰',\n",
              " '히',\n",
              " '힌',\n",
              " '힘'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 글자 집합의 크기\n",
        "src_vocab_size = len(src_vocab)+1\n",
        "tar_vocab_size = len(tar_vocab)+1\n",
        "print('source 문장의 char 집합 :',src_vocab_size)\n",
        "print('target 문장의 char 집합 :',tar_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgplnvZLIxoQ",
        "outputId": "593467ce-7b79-4216-be7e-1cecf289c894"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source 문장의 char 집합 : 75\n",
            "target 문장의 char 집합 : 914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "print(src_vocab[45:75])\n",
        "print(tar_vocab[45:75])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUBIE-4RI1fi",
        "outputId": "0a9b23a7-9a19-4c56-f718-8e6c859279a9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'ï']\n",
            "['간', '갇', '갈', '감', '갑', '값', '갔', '강', '갖', '같', '개', '객', '갰', '걀', '걔', '거', '걱', '건', '걷', '걸', '검', '겁', '것', '게', '겐', '겠', '겨', '격', '겪', '견']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 글자에 인덱스를 부여\n",
        "\n",
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "print(src_to_index)\n",
        "print(tar_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8F9bZWJ3ey",
        "outputId": "12aed37b-951d-4eb6-ea57-6ac3ef6c99a6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'Y': 46, 'a': 47, 'b': 48, 'c': 49, 'd': 50, 'e': 51, 'f': 52, 'g': 53, 'h': 54, 'i': 55, 'j': 56, 'k': 57, 'l': 58, 'm': 59, 'n': 60, 'o': 61, 'p': 62, 'q': 63, 'r': 64, 's': 65, 't': 66, 'u': 67, 'v': 68, 'w': 69, 'x': 70, 'y': 71, 'z': 72, '°': 73, 'ï': 74}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '%': 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'H': 29, 'M': 30, 'N': 31, 'T': 32, 'a': 33, 'd': 34, 'h': 35, 'i': 36, 'm': 37, 'o': 38, 'p': 39, 'r': 40, 't': 41, 'y': 42, '°': 43, '가': 44, '각': 45, '간': 46, '갇': 47, '갈': 48, '감': 49, '갑': 50, '값': 51, '갔': 52, '강': 53, '갖': 54, '같': 55, '개': 56, '객': 57, '갰': 58, '걀': 59, '걔': 60, '거': 61, '걱': 62, '건': 63, '걷': 64, '걸': 65, '검': 66, '겁': 67, '것': 68, '게': 69, '겐': 70, '겠': 71, '겨': 72, '격': 73, '겪': 74, '견': 75, '결': 76, '겼': 77, '경': 78, '계': 79, '고': 80, '곡': 81, '곤': 82, '곧': 83, '골': 84, '곰': 85, '곱': 86, '곳': 87, '공': 88, '과': 89, '관': 90, '광': 91, '괜': 92, '괴': 93, '굉': 94, '교': 95, '구': 96, '국': 97, '군': 98, '굳': 99, '굴': 100, '굶': 101, '굼': 102, '굽': 103, '궁': 104, '권': 105, '귀': 106, '귄': 107, '규': 108, '그': 109, '극': 110, '근': 111, '글': 112, '금': 113, '급': 114, '긋': 115, '긍': 116, '기': 117, '긴': 118, '길': 119, '깊': 120, '까': 121, '깎': 122, '깐': 123, '깔': 124, '깜': 125, '깡': 126, '깨': 127, '꺼': 128, '꺾': 129, '껍': 130, '껏': 131, '껐': 132, '께': 133, '껴': 134, '꼈': 135, '꼬': 136, '꼴': 137, '꼼': 138, '꽃': 139, '꽉': 140, '꽤': 141, '꾸': 142, '꾼': 143, '꿇': 144, '꿈': 145, '꿔': 146, '꿨': 147, '뀌': 148, '끄': 149, '끈': 150, '끊': 151, '끌': 152, '끓': 153, '끔': 154, '끗': 155, '끙': 156, '끝': 157, '끼': 158, '낀': 159, '낄': 160, '낌': 161, '나': 162, '낙': 163, '낚': 164, '난': 165, '날': 166, '낡': 167, '남': 168, '납': 169, '났': 170, '낭': 171, '낮': 172, '낯': 173, '내': 174, '낸': 175, '낼': 176, '냄': 177, '냈': 178, '냉': 179, '냐': 180, '냥': 181, '너': 182, '넌': 183, '널': 184, '넓': 185, '넘': 186, '넛': 187, '넣': 188, '네': 189, '넷': 190, '녀': 191, '녁': 192, '년': 193, '념': 194, '녕': 195, '노': 196, '녹': 197, '논': 198, '놀': 199, '농': 200, '높': 201, '놓': 202, '놔': 203, '놨': 204, '뇌': 205, '누': 206, '눅': 207, '눈': 208, '눴': 209, '뉴': 210, '늄': 211, '느': 212, '늑': 213, '는': 214, '늘': 215, '늙': 216, '능': 217, '늦': 218, '니': 219, '닌': 220, '님': 221, '다': 222, '닥': 223, '닦': 224, '단': 225, '닫': 226, '달': 227, '닮': 228, '담': 229, '답': 230, '당': 231, '대': 232, '댔': 233, '더': 234, '덕': 235, '던': 236, '덜': 237, '덤': 238, '덥': 239, '데': 240, '덴': 241, '도': 242, '독': 243, '돈': 244, '돌': 245, '돕': 246, '동': 247, '돼': 248, '됐': 249, '되': 250, '된': 251, '될': 252, '됩': 253, '두': 254, '둑': 255, '둔': 256, '둘': 257, '둠': 258, '둬': 259, '뒀': 260, '뒤': 261, '뒷': 262, '드': 263, '득': 264, '든': 265, '듣': 266, '들': 267, '듯': 268, '등': 269, '디': 270, '딘': 271, '딨': 272, '따': 273, '딱': 274, '딸': 275, '땅': 276, '땋': 277, '때': 278, '떠': 279, '떡': 280, '떤': 281, '떨': 282, '떴': 283, '떻': 284, '또': 285, '똑': 286, '뚱': 287, '뛰': 288, '뜨': 289, '뜰': 290, '뜻': 291, '라': 292, '락': 293, '란': 294, '랄': 295, '람': 296, '랍': 297, '랐': 298, '랑': 299, '래': 300, '랜': 301, '램': 302, '랩': 303, '랬': 304, '략': 305, '량': 306, '러': 307, '럭': 308, '런': 309, '럴': 310, '럼': 311, '럽': 312, '렀': 313, '렁': 314, '렇': 315, '레': 316, '렌': 317, '렛': 318, '려': 319, '력': 320, '련': 321, '렵': 322, '렸': 323, '령': 324, '례': 325, '로': 326, '록': 327, '론': 328, '롭': 329, '뢰': 330, '료': 331, '루': 332, '룹': 333, '류': 334, '륙': 335, '륜': 336, '륭': 337, '르': 338, '른': 339, '를': 340, '름': 341, '릅': 342, '릎': 343, '리': 344, '린': 345, '릴': 346, '림': 347, '립': 348, '마': 349, '막': 350, '만': 351, '많': 352, '말': 353, '맙': 354, '맛': 355, '망': 356, '맞': 357, '맡': 358, '매': 359, '맥': 360, '맨': 361, '맷': 362, '머': 363, '먹': 364, '먼': 365, '멀': 366, '멈': 367, '멋': 368, '멍': 369, '메': 370, '멕': 371, '멜': 372, '며': 373, '면': 374, '명': 375, '몇': 376, '모': 377, '목': 378, '몰': 379, '몸': 380, '못': 381, '묘': 382, '무': 383, '묵': 384, '묶': 385, '문': 386, '묻': 387, '물': 388, '뭇': 389, '뭐': 390, '뭔': 391, '뭘': 392, '므': 393, '미': 394, '민': 395, '믿': 396, '밀': 397, '밌': 398, '밍': 399, '밑': 400, '바': 401, '박': 402, '밖': 403, '반': 404, '받': 405, '발': 406, '밝': 407, '밟': 408, '밤': 409, '밥': 410, '방': 411, '배': 412, '백': 413, '뱀': 414, '버': 415, '벅': 416, '번': 417, '벌': 418, '범': 419, '법': 420, '벗': 421, '벙': 422, '베': 423, '벼': 424, '벽': 425, '변': 426, '별': 427, '병': 428, '보': 429, '복': 430, '본': 431, '볼': 432, '봄': 433, '봅': 434, '봇': 435, '봉': 436, '봐': 437, '봤': 438, '부': 439, '북': 440, '분': 441, '불': 442, '붉': 443, '붐': 444, '붕': 445, '붙': 446, '브': 447, '블': 448, '비': 449, '빈': 450, '빌': 451, '빙': 452, '빛': 453, '빠': 454, '빨': 455, '빴': 456, '빵': 457, '빼': 458, '뺄': 459, '뻐': 460, '뻔': 461, '뻤': 462, '뽀': 463, '뽑': 464, '뿌': 465, '뿐': 466, '쁘': 467, '쁜': 468, '쁠': 469, '삐': 470, '사': 471, '삭': 472, '산': 473, '살': 474, '삶': 475, '삼': 476, '샀': 477, '상': 478, '새': 479, '색': 480, '샌': 481, '생': 482, '샤': 483, '샴': 484, '서': 485, '석': 486, '선': 487, '설': 488, '섬': 489, '섭': 490, '섯': 491, '섰': 492, '성': 493, '세': 494, '센': 495, '셀': 496, '셈': 497, '셔': 498, '셜': 499, '셨': 500, '소': 501, '속': 502, '손': 503, '솔': 504, '송': 505, '쇠': 506, '수': 507, '숙': 508, '순': 509, '숟': 510, '술': 511, '숨': 512, '쉬': 513, '쉽': 514, '슈': 515, '스': 516, '슨': 517, '슬': 518, '습': 519, '승': 520, '시': 521, '식': 522, '신': 523, '실': 524, '싫': 525, '심': 526, '십': 527, '싱': 528, '싶': 529, '싸': 530, '쌉': 531, '써': 532, '썩': 533, '썼': 534, '썽': 535, '쏘': 536, '쏠': 537, '쏴': 538, '쓰': 539, '쓱': 540, '쓴': 541, '쓸': 542, '씀': 543, '씨': 544, '씩': 545, '씬': 546, '씻': 547, '아': 548, '악': 549, '안': 550, '앉': 551, '않': 552, '알': 553, '앓': 554, '암': 555, '압': 556, '앗': 557, '았': 558, '앙': 559, '앞': 560, '애': 561, '액': 562, '앨': 563, '앵': 564, '야': 565, '약': 566, '얀': 567, '얇': 568, '양': 569, '얗': 570, '얘': 571, '어': 572, '억': 573, '언': 574, '얻': 575, '얼': 576, '엄': 577, '업': 578, '없': 579, '엇': 580, '었': 581, '엌': 582, '에': 583, '엔': 584, '엘': 585, '여': 586, '역': 587, '연': 588, '열': 589, '염': 590, '엽': 591, '였': 592, '영': 593, '옆': 594, '예': 595, '옛': 596, '오': 597, '옥': 598, '온': 599, '올': 600, '옮': 601, '옳': 602, '옷': 603, '옹': 604, '와': 605, '완': 606, '왔': 607, '왜': 608, '외': 609, '왼': 610, '요': 611, '욕': 612, '용': 613, '우': 614, '운': 615, '울': 616, '움': 617, '웃': 618, '워': 619, '원': 620, '월': 621, '웠': 622, '웨': 623, '위': 624, '윈': 625, '윗': 626, '윙': 627, '유': 628, '육': 629, '윤': 630, '으': 631, '은': 632, '을': 633, '음': 634, '읍': 635, '응': 636, '의': 637, '이': 638, '익': 639, '인': 640, '일': 641, '읽': 642, '잃': 643, '임': 644, '입': 645, '있': 646, '잊': 647, '자': 648, '작': 649, '잔': 650, '잖': 651, '잘': 652, '잠': 653, '잡': 654, '잤': 655, '장': 656, '재': 657, '잭': 658, '쟁': 659, '저': 660, '적': 661, '전': 662, '절': 663, '젊': 664, '점': 665, '접': 666, '정': 667, '제': 668, '젝': 669, '젠': 670, '젯': 671, '져': 672, '졌': 673, '조': 674, '족': 675, '존': 676, '졸': 677, '좀': 678, '종': 679, '좋': 680, '좌': 681, '죄': 682, '죠': 683, '주': 684, '죽': 685, '준': 686, '줄': 687, '중': 688, '줘': 689, '줬': 690, '쥐': 691, '즈': 692, '즉': 693, '즐': 694, '즘': 695, '증': 696, '지': 697, '직': 698, '진': 699, '질': 700, '집': 701, '짓': 702, '짖': 703, '짜': 704, '짝': 705, '짧': 706, '째': 707, '쨌': 708, '쩔': 709, '쩡': 710, '쪄': 711, '쪘': 712, '쪼': 713, '쪽': 714, '쫓': 715, '쯤': 716, '찌': 717, '찍': 718, '찔': 719, '찡': 720, '찢': 721, '차': 722, '착': 723, '찬': 724, '찮': 725, '찰': 726, '참': 727, '찼': 728, '창': 729, '찾': 730, '채': 731, '책': 732, '챌': 733, '챘': 734, '처': 735, '척': 736, '천': 737, '철': 738, '첫': 739, '청': 740, '체': 741, '쳐': 742, '쳤': 743, '초': 744, '촌': 745, '총': 746, '최': 747, '추': 748, '축': 749, '출': 750, '춤': 751, '충': 752, '춰': 753, '췄': 754, '취': 755, '츠': 756, '측': 757, '치': 758, '칙': 759, '친': 760, '칠': 761, '침': 762, '칩': 763, '카': 764, '캐': 765, '커': 766, '컨': 767, '컴': 768, '컵': 769, '케': 770, '켓': 771, '켜': 772, '켤': 773, '켰': 774, '코': 775, '콜': 776, '콥': 777, '콩': 778, '쾅': 779, '쿠': 780, '쿨': 781, '퀴': 782, '큐': 783, '크': 784, '큰': 785, '큼': 786, '키': 787, '킬': 788, '킹': 789, '타': 790, '탁': 791, '탄': 792, '탈': 793, '탐': 794, '탑': 795, '탓': 796, '탔': 797, '탕': 798, '태': 799, '택': 800, '터': 801, '턱': 802, '턴': 803, '테': 804, '텐': 805, '토': 806, '톤': 807, '톰': 808, '톱': 809, '통': 810, '퇴': 811, '투': 812, '트': 813, '특': 814, '튼': 815, '틀': 816, '티': 817, '틱': 818, '틴': 819, '팀': 820, '팅': 821, '파': 822, '판': 823, '팔': 824, '팠': 825, '패': 826, '퍼': 827, '펐': 828, '페': 829, '펙': 830, '펜': 831, '펭': 832, '펴': 833, '편': 834, '평': 835, '폐': 836, '포': 837, '폭': 838, '폰': 839, '표': 840, '푸': 841, '푹': 842, '풀': 843, '품': 844, '풍': 845, '퓨': 846, '프': 847, '픈': 848, '플': 849, '픔': 850, '피': 851, '필': 852, '핑': 853, '하': 854, '학': 855, '한': 856, '할': 857, '함': 858, '합': 859, '항': 860, '해': 861, '핸': 862, '햄': 863, '했': 864, '행': 865, '향': 866, '허': 867, '헉': 868, '헌': 869, '험': 870, '헤': 871, '헬': 872, '헷': 873, '혀': 874, '현': 875, '혈': 876, '혐': 877, '협': 878, '혔': 879, '형': 880, '혜': 881, '호': 882, '혹': 883, '혼': 884, '홀': 885, '홋': 886, '화': 887, '확': 888, '환': 889, '활': 890, '황': 891, '회': 892, '획': 893, '효': 894, '후': 895, '훈': 896, '훌': 897, '훔': 898, '훨': 899, '휘': 900, '휴': 901, '흐': 902, '흔': 903, '흘': 904, '흙': 905, '흠': 906, '흡': 907, '흥': 908, '희': 909, '흰': 910, '히': 911, '힌': 912, '힘': 913}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 영어 문장에 대한 정수 인코딩을 수행\n",
        "\n",
        "encoder_input = []\n",
        "\n",
        "# 1개의 문장\n",
        "for line in lines.src:  # 한개의 문장을 뽑아낸다. \n",
        "  encoded_line = []\n",
        "  # 각 줄에서 1개의 char\n",
        "  for char in line:  # 한 개의 문장에 대해 정수인코딩.\n",
        "    # 각 char을 정수로 변환\n",
        "    encoded_line.append(src_to_index[char])\n",
        "  encoder_input.append(encoded_line)   # 정수인코딩된 문장을 인코더input리스트에 넣는다. \n",
        "print('source 문장의 정수 인코딩 :',encoder_input[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an6UMBtSKBjK",
        "outputId": "f19605ce-59ac-4f04-a2ce-20cb1fc2a056"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source 문장의 정수 인코딩 : [[29, 61, 9], [30, 55, 9], [40, 67, 60, 2], [40, 67, 60, 9], [45, 54, 61, 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) 한국어 타겟데이터에 대해서 정수 인코딩\n",
        "decoder_input = []\n",
        "for line in lines.tar:\n",
        "  encoded_line = []\n",
        "  for char in line:\n",
        "    encoded_line.append(tar_to_index[char])\n",
        "  decoder_input.append(encoded_line)\n",
        "print('target 문장의 정수 인코딩 :',decoder_input[:5])\n",
        "\n",
        "# 시작 심볼, 종료심볼인 1과 2로 감싸져 있음을 볼수 있음."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcKICmYjKk2X",
        "outputId": "6b3e345c-57ae-4a87-b9e6-fde5ac38b2fc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target 문장의 정수 인코딩 : [[1, 3, 44, 11, 3, 2], [1, 3, 550, 195, 11, 3, 2], [1, 3, 288, 572, 4, 3, 2], [1, 3, 288, 572, 11, 3, 2], [1, 3, 206, 96, 24, 3, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  3) 디코더의 예측값과 비교하기 위한 실제값 정수인코딩(시작심볼이 필요없음.-> 문장의 맨 앞에 붙어있는 '\\t'를 제거)\n",
        "decoder_target = []\n",
        "for line in lines.tar:\n",
        "  timestep = 0\n",
        "  encoded_line = []\n",
        "  for char in line:\n",
        "    if timestep > 0:\n",
        "      encoded_line.append(tar_to_index[char])\n",
        "    timestep = timestep + 1\n",
        "  decoder_target.append(encoded_line)\n",
        "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsM2RpsiLBrr",
        "outputId": "1dc847fe-e896-43bf-9d2b-662572258a86"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target 문장 레이블의 정수 인코딩 : [[3, 44, 11, 3, 2], [3, 550, 195, 11, 3, 2], [3, 288, 572, 4, 3, 2], [3, 288, 572, 11, 3, 2], [3, 206, 96, 24, 3, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 설계하기 전에 혹시 의아한 점은 없으신가요? 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 설명하였는데 decoder_input이 왜 필요할까요?\n",
        "\n",
        "훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."
      ],
      "metadata": {
        "id": "17r4q4XlPBgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 확인합니다.\n",
        "max_src_len = max([len(line) for line in lines.src])\n",
        "max_tar_len = max([len(line) for line in lines.tar])\n",
        "print('source 문장의 최대 길이 :',max_src_len)\n",
        "print('target 문장의 최대 길이 :',max_tar_len)\n",
        "\n",
        "# 영어와 프랑스어의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 \n",
        "# 패딩을 할 때도 이 두 개의 데이터의 길이를 전부 동일하게 맞춰줄 필요는 없습니다. 영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞추어서 패딩하면 됩니다. \n",
        "# 여기서는 가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 23이 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfFjHenlNJdl",
        "outputId": "8d1983ef-e874-46b5-fc0f-565ae8225580"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source 문장의 최대 길이 : 537\n",
            "target 문장의 최대 길이 : 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line in lines.tar:\n",
        "    if len(line) == max_tar_len:\n",
        "        print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG74V4K-N7Dx",
        "outputId": "09eb118a-2ddb-4469-c886-a5b4e6edda34"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지. 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면, 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고, 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면, 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면, 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line in lines.src:\n",
        "    if len(line) == max_src_len:\n",
        "        print(line)\n",
        "\n",
        "# 문제의 원인!\n",
        "# 아마 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생떄문에 번역이 제대로 안된 것 같음.\n",
        "# 그것도 그렇고... 영어와 불어라면 모를까, 한국어와 영어는 어휘체계가 달라서 글자 단위가 아닌 단어단위로 해야할 것 같음. --> 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상 --> 어텐션 등장.\n",
        "# https://wikidocs.net/86900"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxipDGD8orCx",
        "outputId": "318e667e-1987-4c48-9ff9-e217e134c0b7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
      ],
      "metadata": {
        "id": "uJ4vcMPsNQIC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 글자 단위 번역기므로 워드 임베딩은 별도로 사용되지 않으며, 예측값과의 오차 측정에 사용되는 실제값뿐만 아니라 입력값도 원-핫 벡터를 사용하겠습니다.\n",
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)"
      ],
      "metadata": {
        "id": "7NfZFXGFOetZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq 모델을 설계 - 교사 강요를 사용하여 훈련\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "h-WxsWLLOode"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  functional API를 사용\n",
        "encoder_inputs = Input(shape=(None, src_vocab_size))  #src_vocab_size : 영어글자 집합의 크기\n",
        "encoder_lstm = LSTM(units=256, return_state=True)  # return_state=True 마지막 시점의 은닉상태출력. 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정\n",
        "\n",
        "# encoder_outputs은 여기서는 불필요,은닉상태, 셀상태만 전달하면 된다. \n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태. LSTM은 은닉 상태와 셀 상태라는 두 가지 상태를 가진다\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "#  encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달합니다. 이것이 앞서 배운 컨텍스트 벡터입니다."
      ],
      "metadata": {
        "id": "xKzYfhY5PIbr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None, tar_vocab_size))  #tar_vocab_size : 한국글자 집합크기\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True) #동일하게 디코더의 은닉 상태 크기도 256\n",
        "\n",
        "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)  # initial_state=encoder_states : 디코더는 인코더의 마지막 은닉 상태를 초기 은닉 상태로 사용합  디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용x\n",
        "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')  #출력층에 한국어의 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 실제값과의 오차를 구합니다.\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "Y7i64xJzPJWt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻습니다.\n",
        "# 2. 상태와 <SOS>에 해당하는 '\\t'를 디코더로 보냅니다.\n",
        "# 3. 디코더가 <EOS>에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다."
      ],
      "metadata": {
        "id": "TF8Rfm1AevvW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
      ],
      "metadata": {
        "id": "P5O_sfLilJhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용\n",
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
        "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "DUS00HU2ltjK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어로부터 인덱스를 얻는 것이 아니라 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만들었습니다.\n",
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
      ],
      "metadata": {
        "id": "21qXv7EpmVau"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 상태를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = \"\"\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "    if (sampled_char == '\\n' or\n",
        "        len(decoded_sentence) > max_tar_len):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "dSsr-tpVmWHw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
        "  input_seq = encoder_input[seq_index:seq_index+1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(35 * \"-\")\n",
        "  print('입력 문장:', lines.src[seq_index])\n",
        "  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "nChgyBz8mhA-",
        "outputId": "2ea5b55d-a9dc-4377-81bf-9598bbf76a91"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "입력 문장: Run.\n",
            "정답 문장: 뛰어. \n",
            "번역 문장: 국,월월월C뉴뉴월월C뉴뉴탓탓탓육육려줬최가가와와와와와와와와셨셨늙셨늙왔챌챌브한챌챌브한마헉는던던비땋년밌깔홋돼aa떴몰뒷왼막읽샌섯뱀됐곧흙곧밑개마람당딱긴몇몇디봤취쪼넌구구씬났!취문짧건은낼모천웠롭롭꿔몰각각d윈과기맡료료료박엔료그그깊닥북북좀껏륙더운1떻냈루월월CC뉴뉴길월월월월C뉴뉴탓탓육육려르능똑찔곡축꿈옷괜삶국쿠섭어무작)매:썼만탁탁평묶묶뛰혼혼헌M늄압코낭젯감혼혼대대송송였였힘컵컵컵폐컵컵폐컵폐컵,밀플\"난덕덕잔청청럽럽줄럽얻꽃특륜뭇젝꿈봤락헤렌내mm휴m휴m품품급읽줘줘줘앙낭mm품급읽랍천천만육자자명묶뛰묶젯묶젯혼헌혼헌등씀씬듯듯특후조황d거허의힌멈화곤곤곤곤텐썼덤현현섭뒤이\n",
            "-----------------------------------\n",
            "입력 문장: Help me.\n",
            "정답 문장: 도와줘. \n",
            "번역 문장: 국,월월월C뉴뉴월월C뉴뉴탓탓탓육육려줬최가가와와와와와와와와셨셨늙셨늙왔챌챌브한챌챌브한마헉는던던비땋년밌깔홋돼aa떴몰뒷왼막읽샌섯뱀됐곧흙곧밑개마람당딱긴몇몇디봤취쪼넌구구씬났!취문짧건은낼모천웠롭롭꿔몰각각d윈과기맡료료료박엔료그그깊닥북북좀껏륙더운1떻냈루월월CC뉴뉴길월월월월C뉴뉴탓탓육육려르능똑찔곡축꿈옷괜삶국쿠섭어무작)매:썼만탁탁평묶묶뛰혼혼헌M늄압코낭젯감혼혼대대송송였였힘컵컵컵폐컵컵폐컵폐컵,밀플\"난덕덕잔청청럽럽줄럽얻꽃특륜뭇젝꿈봤락헤렌내mm휴m휴m품품급읽줘줘줘앙낭mm품급읽랍천천만육자자명묶뛰묶젯묶젯혼헌혼헌등씀씬듯듯특후조황d거허의힌멈화곤곤곤곤텐썼덤현현섭뒤이\n",
            "-----------------------------------\n",
            "입력 문장: Tom lost.\n",
            "정답 문장: 톰이 졌어. \n",
            "번역 문장: 국,월월월C뉴뉴월월C뉴뉴탓탓탓육육려줬최가가와와와와와와와와셨셨늙셨늙왔챌챌브한챌챌브한마헉는던던비땋년밌깔홋돼aa떴몰뒷왼막읽샌섯뱀됐곧흙곧밑개마람당딱긴몇몇디봤취쪼넌구구씬났!취문짧건은낼모천웠롭롭꿔몰각각d윈과기맡료료료박엔료그그깊닥북북좀껏륙더운1떻냈루월월CC뉴뉴길월월월월C뉴뉴탓탓육육려르능똑찔곡축꿈옷괜삶국쿠섭어무작)매:썼만탁탁평묶묶뛰혼혼헌M늄압코낭젯감혼혼대대송송였였힘컵컵컵폐컵컵폐컵폐컵,밀플\"난덕덕잔청청럽럽줄럽얻꽃특륜뭇젝꿈봤락헤렌내mm휴m휴m품품급읽줘줘줘앙낭mm품급읽랍천천만육자자명묶뛰묶젯묶젯혼헌혼헌등씀씬듯듯특후조황d거허의힌멈화곤곤곤곤텐썼덤현현섭뒤이\n",
            "-----------------------------------\n",
            "입력 문장: Come closer.\n",
            "정답 문장: 가까이 와. \n",
            "번역 문장: 국,월월월C뉴뉴월월C뉴뉴탓탓탓육육려줬최가가와와와와와와와와셨셨늙셨늙왔챌챌브한챌챌브한마헉는던던비땋년밌깔홋돼aa떴몰뒷왼막읽샌섯뱀됐곧흙곧밑개마람당딱긴몇몇디봤취쪼넌구구씬났!취문짧건은낼모천웠롭롭꿔몰각각d윈과기맡료료료박엔료그그깊닥북북좀껏륙더운1떻냈루월월CC뉴뉴길월월월월C뉴뉴탓탓육육려르능똑찔곡축꿈옷괜삶국쿠섭어무작)매:썼만탁탁평묶묶뛰혼혼헌M늄압코낭젯감혼혼대대송송였였힘컵컵컵폐컵컵폐컵폐컵,밀플\"난덕덕잔청청럽럽줄럽얻꽃특륜뭇젝꿈봤락헤렌내mm휴m휴m품품급읽줘줘줘앙낭mm품급읽랍천천만육자자명묶뛰묶젯묶젯혼헌혼헌등씀씬듯듯특후조황d거허의힌멈화곤곤곤곤텐썼덤현현섭뒤이\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e863f353130c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 입력 문장의 인덱스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'입력 문장:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-6583fde7dc1f>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 예측 결과를 문자로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     dataset = tf.data.Dataset.zip((\n\u001b[1;32m    352\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     ))\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    699\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \"\"\"\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m   4632\u001b[0m   \u001b[0;34m\"\"\"A `Dataset` with a single element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4634\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4635\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 어텐션\n",
        "- 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됩니다.\n",
        "\n",
        "- Attention(Q, K, V) = Attention Value\n",
        "\n",
        "- 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영해줍니다. 그리고 유사도가 반영된 '값(Value)'을 모두 더해서 리턴합니다. 여기서는 이를 어텐션 값(Attention Value)이라고 하겠습니다.\n",
        "\n",
        "```\n",
        "Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n",
        "K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "```"
      ],
      "metadata": {
        "id": "EIE5SGI6nk0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머(Transformer)\n",
        "https://wikidocs.net/31379\n",
        "\n",
        "- \"Attention is all you need\"에서 나온 모델로 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델\n",
        "\n",
        "이 모델은 RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 성능도 RNN보다 우수하다는 특징을 갖고있습니다.\n",
        "\n",
        "\n",
        " ```\n",
        " dmodel = 512\n",
        "트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기를 의미합니다. 임베딩 벡터의 차원 또한 \n",
        "이며, 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지합니다. 논문에서는 512입니다.\n",
        "\n",
        "num_layers= 6\n",
        "트랜스포머에서 하나의 인코더와 디코더를 층으로 생각하였을 때, 트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지를 의미합니다. 논문에서는 인코더와 디코더를 각각 총 6개 쌓았습니다.\n",
        "\n",
        " num_heads= 8\n",
        "트랜스포머에서는 어텐션을 사용할 때, 1번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택했습니다. 이때 이 병렬의 개수를 의미합니다.\n",
        "\n",
        " dff= 2048\n",
        "트랜스포머 내부에는 피드 포워드 신경망이 존재합니다. 이때 은닉층의 크기를 의미합니다. 피드 포워드 신경망의 입력층과 출력층의 크기는 \n",
        "입니다.\n",
        "```\n",
        "\n",
        "이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time-step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조입니다. 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개를 사용하였습니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "htwG-AIz60IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 트랜스포머의 입력: 포지셔널 인코딩(Positional Encoding)\n",
        "\n",
        "- RNN이 자연어 처리에서 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보(position information)를 가질 수 있다는 점\n",
        "\n",
        "- 하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있습니다. 트랜스포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩(positional encoding)이라고 합니다.\n",
        "- 결국 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터라고 보면 되겠습니다.\n"
      ],
      "metadata": {
        "id": "0kyazMU0CeAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 어텐션(Attention)\n",
        "\n",
        "트랜스포머에서 사용되는 세 가지의 어텐션\n",
        "- 인코더의 셀프 어텐션 : Query = Key = Value\n",
        "- 디코더의 마스크드 셀프 어텐션 : Query = Key = Value\n",
        "- 디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터"
      ],
      "metadata": {
        "id": "pThu_qYDD-wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 인코더(Encoder)\n",
        "- 트랜스포머는 하이퍼파라미터인  개수의 인코더 층을 쌓습니다. 논문에서는 총 6개의 인코더 층을 사용하였습니다.\n",
        "- 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어집니다. 바로 셀프 어텐션과 피드 포워드 신경망입니다. \n",
        "- 멀티 헤드 셀프 어텐션은 셀프 어텐션을 병렬적으로 사용하였다는 의미고, 포지션 와이즈 피드 포워드 신경망은 우리가 알고있는 일반적인 피드 포워드 신경망\n"
      ],
      "metadata": {
        "id": "Bj74SevWFB0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인코더의 셀프 어텐션\n",
        "-  셀프 어텐션이 앞서 배웠던 어텐션과 무엇이 다른지 이해해보겠습니다.\n",
        "\n",
        "- 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 그리고 구해낸 이 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 '값(Value)'에 반영해줍니다. 그리고 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴합니다.\n",
        "\n",
        "- 여기까지는 앞서 배운 어텐션의 개념입니다. 그런데 어텐션 중에서는 셀프 어텐션(self-attention)이라는 것이 있습니다. 단지 어텐션을 자기 자신에게 수행한다는 의미입니다. 앞서 배운 seq2seq에서 어텐션을 사용할 경우의 Q, K, V의 정의를 다시 생각해봅시다.\n",
        "\n",
        "```\n",
        "Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n",
        "K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "```\n",
        "\n",
        "그런데 사실 t 시점이라는 것은 계속 변화하면서 반복적으로 쿼리를 수행하므로 결국 전체 시점에 대해서 일반화를 할 수도 있습니다.\n",
        "\n",
        "```\n",
        "Q = Querys : 모든 시점의 디코더 셀에서의 은닉 상태들\n",
        "K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n",
        "```\n",
        "\n",
        "이처럼 기존에는 디코더 셀의 은닉 상태가 Q이고 인코더 셀의 은닉 상태가 K라는 점에서 Q와 K가 서로 다른 값을 가지고 있었습니다. 그런데 셀프 어텐션에서는 Q, K, V가 전부 동일합니다. 트랜스포머의 셀프 어텐션에서의 Q, K, V는 아래와 같습니다.\n",
        "\n",
        "```\n",
        "Q : 입력 문장의 모든 단어 벡터들\n",
        "K : 입력 문장의 모든 단어 벡터들\n",
        "V : 입력 문장의 모든 단어 벡터들\n",
        "```\n",
        "\n",
        "\n",
        "이후생략\n",
        "https://wikidocs.net/31379\n",
        "\n",
        "ㅋㅋㅋ 이게머야 ㅋㅋㅋ\n",
        "wow Wow WWOWWWWWWW LOOLOOLOLOLOLOL"
      ],
      "metadata": {
        "id": "bdYxSRISFnmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "-qIR0dOz5Jd4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}